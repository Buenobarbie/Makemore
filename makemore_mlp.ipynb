{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "chars = [c for c in '.abcdefghijklmnopqrstuvwxyz']\n",
    "\n",
    "# Convertion between characters and integers\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "stoi = {c: i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # how many characters do we take to predict the next one\n",
    "\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "    for c in w + '.':\n",
    "        X.append(context.copy())\n",
    "        Y.append(stoi[c])\n",
    "        context = context[1:] + [stoi[c]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0],\n",
       " [0, 0, 5],\n",
       " [0, 5, 13],\n",
       " [5, 13, 13],\n",
       " [13, 13, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 15],\n",
       " [0, 15, 12],\n",
       " [15, 12, 9],\n",
       " [12, 9, 22],\n",
       " [9, 22, 9],\n",
       " [22, 9, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 1, 22],\n",
       " [1, 22, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 9],\n",
       " [0, 9, 19],\n",
       " [9, 19, 1],\n",
       " [19, 1, 2],\n",
       " [1, 2, 5],\n",
       " [2, 5, 12],\n",
       " [5, 12, 12],\n",
       " [12, 12, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 19],\n",
       " [0, 19, 15],\n",
       " [19, 15, 16],\n",
       " [15, 16, 8],\n",
       " [16, 8, 9],\n",
       " [8, 9, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 3],\n",
       " [0, 3, 8],\n",
       " [3, 8, 1],\n",
       " [8, 1, 18],\n",
       " [1, 18, 12],\n",
       " [18, 12, 15],\n",
       " [12, 15, 20],\n",
       " [15, 20, 20],\n",
       " [20, 20, 5],\n",
       " [0, 0, 0],\n",
       " [0, 0, 13],\n",
       " [0, 13, 9],\n",
       " [13, 9, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 1, 13],\n",
       " [1, 13, 5],\n",
       " [13, 5, 12],\n",
       " [5, 12, 9],\n",
       " [12, 9, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 8],\n",
       " [0, 8, 1],\n",
       " [8, 1, 18],\n",
       " [1, 18, 16],\n",
       " [18, 16, 5],\n",
       " [16, 5, 18],\n",
       " [0, 0, 0],\n",
       " [0, 0, 5],\n",
       " [0, 5, 22],\n",
       " [5, 22, 5],\n",
       " [22, 5, 12],\n",
       " [5, 12, 25],\n",
       " [12, 25, 14],\n",
       " [0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 1, 2],\n",
       " [1, 2, 9],\n",
       " [2, 9, 7],\n",
       " [9, 7, 1],\n",
       " [7, 1, 9],\n",
       " [1, 9, 12],\n",
       " [0, 0, 0],\n",
       " [0, 0, 5],\n",
       " [0, 5, 13],\n",
       " [5, 13, 9],\n",
       " [13, 9, 12],\n",
       " [9, 12, 25],\n",
       " [0, 0, 0],\n",
       " [0, 0, 5],\n",
       " [0, 5, 12],\n",
       " [5, 12, 9],\n",
       " [12, 9, 26],\n",
       " [9, 26, 1],\n",
       " [26, 1, 2],\n",
       " [1, 2, 5],\n",
       " [2, 5, 20],\n",
       " [5, 20, 8],\n",
       " [0, 0, 0],\n",
       " [0, 0, 13],\n",
       " [0, 13, 9],\n",
       " [13, 9, 12],\n",
       " [9, 12, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 5],\n",
       " [0, 5, 12],\n",
       " [5, 12, 12],\n",
       " [12, 12, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 1, 22],\n",
       " [1, 22, 5],\n",
       " [22, 5, 18],\n",
       " [5, 18, 25],\n",
       " [0, 0, 0],\n",
       " [0, 0, 19],\n",
       " [0, 19, 15],\n",
       " [19, 15, 6],\n",
       " [15, 6, 9],\n",
       " [6, 9, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 3],\n",
       " [0, 3, 1],\n",
       " [3, 1, 13],\n",
       " [1, 13, 9],\n",
       " [13, 9, 12],\n",
       " [9, 12, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 1, 18],\n",
       " [1, 18, 9],\n",
       " [18, 9, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 19],\n",
       " [0, 19, 3],\n",
       " [19, 3, 1],\n",
       " [3, 1, 18],\n",
       " [1, 18, 12],\n",
       " [18, 12, 5],\n",
       " [12, 5, 20],\n",
       " [5, 20, 20],\n",
       " [0, 0, 0],\n",
       " [0, 0, 22],\n",
       " [0, 22, 9],\n",
       " [22, 9, 3],\n",
       " [9, 3, 20],\n",
       " [3, 20, 15],\n",
       " [20, 15, 18],\n",
       " [15, 18, 9],\n",
       " [18, 9, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 13],\n",
       " [0, 13, 1],\n",
       " [13, 1, 4],\n",
       " [1, 4, 9],\n",
       " [4, 9, 19],\n",
       " [9, 19, 15],\n",
       " [19, 15, 14],\n",
       " [0, 0, 0],\n",
       " [0, 0, 12],\n",
       " [0, 12, 21],\n",
       " [12, 21, 14],\n",
       " [21, 14, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 7],\n",
       " [0, 7, 18],\n",
       " [7, 18, 1],\n",
       " [18, 1, 3],\n",
       " [1, 3, 5],\n",
       " [0, 0, 0],\n",
       " [0, 0, 3],\n",
       " [0, 3, 8],\n",
       " [3, 8, 12],\n",
       " [8, 12, 15],\n",
       " [12, 15, 5],\n",
       " [0, 0, 0],\n",
       " [0, 0, 16],\n",
       " [0, 16, 5],\n",
       " [16, 5, 14],\n",
       " [5, 14, 5],\n",
       " [14, 5, 12],\n",
       " [5, 12, 15],\n",
       " [12, 15, 16],\n",
       " [15, 16, 5],\n",
       " [0, 0, 0],\n",
       " [0, 0, 12],\n",
       " [0, 12, 1],\n",
       " [12, 1, 25],\n",
       " [1, 25, 12],\n",
       " [25, 12, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 18],\n",
       " [0, 18, 9],\n",
       " [18, 9, 12],\n",
       " [9, 12, 5],\n",
       " [12, 5, 25],\n",
       " [0, 0, 0],\n",
       " [0, 0, 26],\n",
       " [0, 26, 15],\n",
       " [26, 15, 5],\n",
       " [15, 5, 25],\n",
       " [0, 0, 0],\n",
       " [0, 0, 14],\n",
       " [0, 14, 15],\n",
       " [14, 15, 18],\n",
       " [15, 18, 1],\n",
       " [0, 0, 0],\n",
       " [0, 0, 12],\n",
       " [0, 12, 9],\n",
       " [12, 9, 12],\n",
       " [9, 12, 25],\n",
       " [0, 0, 0],\n",
       " [0, 0, 5],\n",
       " [0, 5, 12],\n",
       " [5, 12, 5],\n",
       " [12, 5, 1],\n",
       " [5, 1, 14],\n",
       " [1, 14, 15],\n",
       " [14, 15, 18]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1177,  0.9859],\n",
       "        [ 0.0065,  0.0346],\n",
       "        [-1.2448, -0.1542],\n",
       "        [-0.8243, -0.2305],\n",
       "        [ 0.7600, -0.9843],\n",
       "        [-2.6133,  0.0392],\n",
       "        [-0.9717,  0.4839],\n",
       "        [-1.4669,  0.2818],\n",
       "        [ 1.6495,  0.5560],\n",
       "        [ 0.6837,  0.3755],\n",
       "        [ 0.1257, -0.0333],\n",
       "        [ 0.2079,  0.7848],\n",
       "        [-0.3290,  1.5316],\n",
       "        [-0.6944,  0.6038],\n",
       "        [ 1.1418,  0.8448],\n",
       "        [-0.5134,  1.0713],\n",
       "        [-1.4933,  0.2535],\n",
       "        [ 0.3746, -0.1736],\n",
       "        [ 0.5028,  0.0682],\n",
       "        [ 0.8618,  1.1734],\n",
       "        [ 2.0097,  0.3080],\n",
       "        [ 0.4396,  0.4407],\n",
       "        [ 1.5504, -0.3431],\n",
       "        [-0.5575, -1.3250],\n",
       "        [ 0.8425, -0.8022],\n",
       "        [ 0.7667,  0.4733],\n",
       "        [-0.1682,  0.9181]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary of size 27 and embbeding of 2 dimensions\n",
    "C = torch.randn((27,2))\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8393, -0.1388])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8393, -0.1388])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8393, -0.1388],\n",
       "        [ 0.3995,  1.0070],\n",
       "        [ 0.5100,  0.9543],\n",
       "        [ 0.5100,  0.9543]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[[5,6,7,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [2, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [2, 1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1607, -0.6618],\n",
       "         [ 0.4185, -2.1254]],\n",
       "\n",
       "        [[ 0.4185, -2.1254],\n",
       "         [ 0.1607, -0.6618]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([212, 3, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer\n",
    "Inputs: 2 dimensional embbedings of 3 previous characters = 6\n",
    "\n",
    "Number of neurons: 100 neurons as exampls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat(torch.unbind(emb,1),1) # returns a list of tensors when we exclude the especified dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = torch.tanh(emb.view(emb.shape[0],6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([212, 100])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Layer\n",
    "input: 100 neurons from the previous layer\n",
    "\n",
    "output: 27 possible characters to come next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn((27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = hidden_state @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [32], [212]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/barbara/Documents/makemore/makemore_mlp.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/barbara/Documents/makemore/makemore_mlp.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mprob[torch\u001b[39m.\u001b[39;49marange(\u001b[39m32\u001b[39;49m), Y]\u001b[39m.\u001b[39mlog()\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/barbara/Documents/makemore/makemore_mlp.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [32], [212]"
     ]
    }
   ],
   "source": [
    "loss = -prob[torch.arange(len(Y)), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 0, 0],\n",
       "  [0, 0, 5],\n",
       "  [0, 5, 13],\n",
       "  [5, 13, 13],\n",
       "  [13, 13, 1],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 15],\n",
       "  [0, 15, 12],\n",
       "  [15, 12, 9],\n",
       "  [12, 9, 22]],\n",
       " [5, 13, 13, 1, 0, 15, 12, 9, 22, 9])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn((100), generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn((27), generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of parameters\n",
    "sum(p.numel() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize learning rate\n",
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  8.463495254516602\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):    \n",
    "    \n",
    "    #mini batch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "\n",
    "    #Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    hidden_state = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1)\n",
    "    logits = hidden_state @ W2 + b2\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data -= p.grad * lr\n",
    "print(\"loss: \" , loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
